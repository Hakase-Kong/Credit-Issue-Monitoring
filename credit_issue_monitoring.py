import nltk

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
import os
from datetime import datetime
import telepot
from openai import OpenAI
import newspaper  # newspaper3k

# --- CSS ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
[data-testid="column"] > div { gap: 0rem !important; }
.stMultiSelect [data-baseweb="tag"] { background-color: #ff5c5c !important; color: white !important; border: none !important; font-weight: bold; }
.sentiment-badge { display: inline-block; padding: 0.08em 0.6em; margin-left: 0.2em; border-radius: 0.8em; font-size: 0.85em; font-weight: bold; vertical-align: middle; }
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em; padding: 1.1em 1.2em 1.2em 1.2em; box-shadow: 0 2px 8px 0 rgba(0,0,0,0.03); }
.flex-row-bottom { display: flex; align-items: flex-end; gap: 0.5rem; margin-bottom: 0.5rem; }
.flex-grow { flex: 1 1 0%; }
.flex-btn { min-width: 90px; }
</style>
""", unsafe_allow_html=True)

# --- ì œì™¸ í‚¤ì›Œë“œ ---
EXCLUDE_TITLE_KEYWORDS = [
    "ì•¼êµ¬", "ì¶•êµ¬", "ë°°êµ¬", "ë†êµ¬", "ê³¨í”„", "eìŠ¤í¬ì¸ ", "ì˜¬ë¦¼í”½", "ì›”ë“œì»µ", "Kë¦¬ê·¸", "í”„ë¡œì•¼êµ¬", "í”„ë¡œì¶•êµ¬", "í”„ë¡œë°°êµ¬", "í”„ë¡œë†êµ¬",
    "ë¶€ê³ ", "ì¸ì‚¬", "ìŠ¹ì§„", "ì„ëª…", "ë°œë ¹", "ì¸ì‚¬ë°œë ¹", "ì¸ì‚¬ì´ë™",
    "ë¸Œëœë“œí‰íŒ", "ë¸Œëœë“œ í‰íŒ", "ë¸Œëœë“œ ìˆœìœ„", "ë¸Œëœë“œì§€ìˆ˜",
    "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì£¼ê°€", "ì£¼ì‹", "ì¦ì‹œ", "ì‹œì„¸", "ë§ˆê°", "ì¥ì¤‘", "ì¥ë§ˆê°", "ê±°ë˜ëŸ‰", "ê±°ë˜ëŒ€ê¸ˆ", "ìƒí•œê°€", "í•˜í•œê°€"
]

def exclude_by_title_keywords(title, exclude_keywords):
    for word in exclude_keywords:
        if word in title:
            return True
    return False

# --- ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™” ---
if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "search_triggered" not in st.session_state:
    st.session_state.search_triggered = False
if "selected_articles" not in st.session_state:
    st.session_state.selected_articles = []

# --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ ---
industry_filter_categories = {
    "ì€í–‰ ë° ê¸ˆìœµì§€ì£¼": {
        "keywords": [
            "ê²½ì˜ì‹¤íƒœí‰ê°€", "BIS", "CET1", "ìë³¸ë¹„ìœ¨", "ìƒê°í˜• ì¡°ê±´ë¶€ìë³¸ì¦ê¶Œ", "ìë³¸í™•ì¶©", "ìë³¸ì—¬ë ¥", "ìë³¸ì ì •ì„±", "LCR",
            "ì¡°ë‹¬ê¸ˆë¦¬", "NIM", "ìˆœì´ìë§ˆì§„", "ê³ ì •ì´í•˜ì—¬ì‹ ë¹„ìœ¨", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ì—°ì²´ìœ¨", "ê°€ê³„ëŒ€ì¶œ", "ì·¨ì•½ì°¨ì£¼"
        ],
        "companies": [
            "ì‹ í•œê¸ˆìœµ", "í•˜ë‚˜ê¸ˆìœµ", "KBê¸ˆìœµ", "ë†í˜‘ê¸ˆìœµ", "ìš°ë¦¬ê¸ˆìœµ",
            "ë†í˜‘ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "ìš°ë¦¬ì€í–‰", "í•˜ë‚˜ì€í–‰"
        ]
    },
    "ë³´í—˜ì‚¬": {
        "keywords": [
            "ë³´ì¥ì„±ë³´í—˜", "ì €ì¶•ì„±ë³´í—˜", "ë³€ì•¡ë³´í—˜", "í‡´ì§ì—°ê¸ˆ", "ì¼ë°˜ë³´í—˜", "ìë™ì°¨ë³´í—˜", "ALM", "ì§€ê¸‰ì—¬ë ¥ë¹„ìœ¨", "K-ICS",
            "ë³´í—˜ìˆ˜ìµì„±", "ë³´í—˜ì†ìµ", "ìˆ˜ì…ë³´í—˜ë£Œ", "CSM", "ìƒê°", "íˆ¬ìì†ìµ", "ìš´ìš©ì„±ê³¼", "IFRS4", "IFRS17", "ë³´í—˜ë¶€ì±„",
            "ì¥ê¸°ì„ ë„ê¸ˆë¦¬", "ìµœì¢…ê´€ì°°ë§Œê¸°", "ìœ ë™ì„± í”„ë¦¬ë¯¸ì—„", "ì‹ ì¢…ìë³¸ì¦ê¶Œ", "í›„ìˆœìœ„ì±„", "ìœ„í—˜ìì‚°ë¹„ì¤‘", "ê°€ì¤‘ë¶€ì‹¤ìì‚°ë¹„ìœ¨"
        ],
        "companies": [
            "í˜„ëŒ€í•´ìƒ", "ë†í˜‘ìƒëª…", "ë©”ë¦¬ì¸ í™”ì¬", "êµë³´ìƒëª…", "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…",
            "ì‹ í•œë¼ì´í”„", "í¥êµ­ìƒëª…", "ë™ì–‘ìƒëª…", "ë¯¸ë˜ì—ì…‹ìƒëª…"
        ]
    },
    # ... ì´í•˜ ë™ì¼í•˜ê²Œ ëª¨ë“  ì„¹í„° ì¶”ê°€ ...
}

# --- ê³µí†µ í•„í„° ì˜µì…˜(ëŒ€ë¶„ë¥˜/ì†Œë¶„ë¥˜ ì—†ì´ ëª¨ë‘ ì ìš©) ---
common_filter_categories = {
    "ì‹ ìš©/ë“±ê¸‰": [
        "ì‹ ìš©ë“±ê¸‰", "ë“±ê¸‰ì „ë§", "í•˜ë½", "ê°•ë“±", "í•˜í–¥", "ìƒí–¥", "ë””í´íŠ¸", "ë¶€ì‹¤", "ë¶€ë„", "ë¯¸ì§€ê¸‰", "ìˆ˜ìš” ë¯¸ë‹¬", "ë¯¸ë§¤ê°", "ì œë„ ê°œí¸", "EOD"
    ],
    "ìˆ˜ìš”/ê³µê¸‰": [
        "ìˆ˜ìš”", "ê³µê¸‰", "ìˆ˜ê¸‰", "ë‘”í™”", "ìœ„ì¶•", "ì„±ì¥", "ê¸‰ë“±", "ê¸‰ë½", "ìƒìŠ¹", "í•˜ë½", "ë¶€ì§„", "ì‹¬í™”"
    ],
    "ì‹¤ì /ì¬ë¬´": [
        "ì‹¤ì ", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ì ì", "ì†ì‹¤", "ë¹„ìš©", "ë¶€ì±„ë¹„ìœ¨", "ì´ìë³´ìƒë°°ìœ¨"
    ],
    "ìê¸ˆ/ì¡°ë‹¬": [
        "ì°¨ì…", "ì¡°ë‹¬", "ì„¤ë¹„íˆ¬ì", "íšŒì‚¬ì±„", "ë°œí–‰", "ì¸ìˆ˜", "ë§¤ê°"
    ],
    "êµ¬ì¡°/ì¡°ì •": [
        "M&A", "í•©ë³‘", "ê³„ì—´ ë¶„ë¦¬", "êµ¬ì¡°ì¡°ì •", "ë‹¤ê°í™”", "êµ¬ì¡° ì¬í¸"
    ],
    "ê±°ì‹œ/ì •ì±…": [
        "ê¸ˆë¦¬", "í™˜ìœ¨", "ê´€ì„¸", "ë¬´ì—­ì œì¬", "ë³´ì¡°ê¸ˆ", "ì„¸ì•¡ ê³µì œ", "ê²½ìŸ"
    ],
    "ì§€ë°°êµ¬ì¡°/ë²•": [
        "íš¡ë ¹", "ë°°ì„", "ê³µì •ê±°ë˜", "ì˜¤ë„ˆë¦¬ìŠ¤í¬", "ëŒ€ì£¼ì£¼", "ì§€ë°°êµ¬ì¡°"
    ]
}
ALL_COMMON_FILTER_KEYWORDS = []
for keywords in common_filter_categories.values():
    ALL_COMMON_FILTER_KEYWORDS.extend(keywords)

# --- UI ì‹œì‘ ---
st.set_page_config(layout="wide")
col_title, col_option1, col_option2 = st.columns([0.6, 0.2, 0.2])
with col_title:
    st.markdown("<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>ğŸ“Š Credit Issue Monitoring</h1>", unsafe_allow_html=True)
with col_option1:
    show_sentiment_badge = st.checkbox("ê¸°ì‚¬ëª©ë¡ì— ê°ì„±ë¶„ì„ ë°°ì§€ í‘œì‹œ", value=False, key="show_sentiment_badge")
with col_option2:
    enable_summary = st.checkbox("ìš”ì•½ ê¸°ëŠ¥ ì ìš©", value=True, key="enable_summary")

# 1. í‚¤ì›Œë“œ ì…ë ¥/ê²€ìƒ‰ ë²„íŠ¼ (í•œ ì¤„, ë²„íŠ¼ ì˜¤ë¥¸ìª½)
col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    keywords_input = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ì‚¼ì„±, í•œí™”)", value="", key="keyword_input", label_visibility="visible")
with col_kw_btn:
    search_clicked = st.button("ê²€ìƒ‰", key="search_btn", help="í‚¤ì›Œë“œë¡œ ê²€ìƒ‰", use_container_width=True)

# ë‚ ì§œ ì…ë ¥
date_col1, date_col2 = st.columns([1, 1])
with date_col1:
    start_date = st.date_input("ì‹œì‘ì¼")
with date_col2:
    end_date = st.date_input("ì¢…ë£Œì¼")

# --- ê³µí†µ í•„í„° ì˜µì…˜ (í•­ìƒ ì ìš©, ì „ì²´ í‚¤ì›Œë“œ ê°€ì‹œì ìœ¼ë¡œ í‘œì‹œ) ---
with st.expander("ğŸ§© ê³µí†µ í•„í„° ì˜µì…˜ (í•­ìƒ ì ìš©ë¨)"):
    for major, subs in common_filter_categories.items():
        st.markdown(f"**{major}**: {', '.join(subs)}")

# --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ (ë©”ì¸ ê²€ìƒ‰ íŠ¸ë¦¬ê±°) ---
with st.expander("ğŸ­ ì‚°ì—…ë³„ í•„í„° ì˜µì…˜"):
    sector_options = list(industry_filter_categories.keys())
    selected_sectors = st.multiselect(
        "ëŒ€ë¶„ë¥˜(ì„¹í„°) ì„ íƒ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥, ì—”í„°ë¡œ ê²€ìƒ‰)",
        sector_options,
        key="industry_majors"
    )

    selected_keywords = sorted(set(
        kw
        for sector in selected_sectors
        for kw in industry_filter_categories[sector]["keywords"]
    )) if selected_sectors else []

    # ì„¹í„° ì„ íƒì´ ë°”ë€Œë©´ ë°”ë¡œ ê²€ìƒ‰ íŠ¸ë¦¬ê±°
    if selected_sectors:
        st.session_state["use_industry_filter"] = True
        st.session_state["industry_sub"] = selected_keywords
        st.session_state["search_triggered"] = True
    else:
        st.session_state["use_industry_filter"] = False
        st.session_state["industry_sub"] = []
        st.session_state["search_triggered"] = False

# --- í‚¤ì›Œë“œ í•„í„° ì˜µì…˜ (í•˜ë‹¨ìœ¼ë¡œ ì´ë™) ---
with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜"):
    require_keyword_in_title = st.checkbox("ê¸°ì‚¬ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ë³´ê¸°", value=False, key="require_keyword_in_title")
    require_exact_keyword_in_title_or_content = st.checkbox("í‚¤ì›Œë“œê°€ ì˜¨ì „íˆ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë³´ê¸°", value=False, key="require_exact_keyword_in_title_or_content")

# --- ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_article_text(url):
    try:
        article = newspaper.article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

# --- OpenAI ìš”ì•½/ê°ì„±ë¶„ì„ í•¨ìˆ˜ ---
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

def summarize_and_sentiment_with_openai(text, do_summary=True):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None, None
    lang = detect_lang(text)
    if lang == "ko":
        prompt = (
            ("ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°ì„±ë¶„ì„(ê¸ì •/ë¶€ì •ë§Œ)í•˜ê³ " +
             ("\n- [í•œ ì¤„ ìš”ì•½]: ê¸°ì‚¬ ì „ì²´ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½" if do_summary else "") +
             "\n- [ê°ì„±]: ê¸°ì‚¬ ì „ì²´ì˜ ê°ì •ì„ ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì¤˜. ì¤‘ë¦½ì€ ì ˆëŒ€ ë‹µí•˜ì§€ ë§ˆ. íŒŒì‚°, ìê¸ˆë‚œ ë“± ë¶€ì •ì  ì‚¬ê±´ì´ ì¤‘ì‹¬ì´ë©´ ë°˜ë“œì‹œ 'ë¶€ì •'ìœ¼ë¡œ ë‹µí•´ì¤˜.\n\n"
             "ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n" +
             ("[í•œ ì¤„ ìš”ì•½]: (ì—¬ê¸°ì— í•œ ì¤„ ìš”ì•½)\n" if do_summary else "") +
             "[ê°ì„±]: (ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜ë§Œ)\n\n"
             "[ê¸°ì‚¬ ë³¸ë¬¸]\n" + text)
        )
    else:
        prompt = (
            ("Analyze the following news article for sentiment (positive/negative only)." +
             ("\n- [One-line Summary]: Summarize the entire article in one sentence." if do_summary else "") +
             "\n- [Sentiment]: Classify the overall sentiment as either positive or negative ONLY. Never answer 'neutral'. If the article is about bankruptcy, crisis, etc., answer 'negative'.\n\n"
             "Respond in this format:\n" +
             ("[One-line Summary]: (your one-line summary)\n" if do_summary else "") +
             "[Sentiment]: (positive/negative only)\n\n"
             "[ARTICLE]\n" + text)
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*(.+)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if (do_summary and m1) else ""
    summary = ""  # ìƒì„¸ ìš”ì•½ì€ ìƒëµ
    sentiment = m3.group(1).strip() if m3 else ""
    # í›„ì²˜ë¦¬: ì¤‘ë¦½ ë“± ë“¤ì–´ì˜¤ë©´ ë¶€ì •ìœ¼ë¡œ ê°•ì œ
    if sentiment.lower() in ['neutral', 'ì¤‘ë¦½', '']:
        sentiment = 'ë¶€ì •' if lang == "ko" else 'negative'
    if lang == "en":
        sentiment = 'ê¸ì •' if sentiment.lower() == 'positive' else 'ë¶€ì •'
    return one_line, summary, sentiment, text

NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(re.sub("<.*?>", "", title), EXCLUDE_TITLE_KEYWORDS):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
        if len(items) < 100:
            break
    return articles[:limit]

def fetch_gnews_news(query, start_date=None, end_date=None, limit=100, require_keyword_in_title=False):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(title, EXCLUDE_TITLE_KEYWORDS):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

# --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° í•¨ìˆ˜ ---
def remove_duplicate_articles(articles):
    seen = set()
    unique_articles = []
    for article in articles:
        link = article.get("link")
        if link and link not in seen:
            unique_articles.append(article)
            seen.add(link)
    return unique_articles

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        else:
            articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        articles = remove_duplicate_articles(articles)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

def detect_lang_from_title(title):
    return "ko" if re.search(r"[ê°€-í£]", title) else "en"

def summarize_article_from_url(article_url, title, do_summary=True):
    try:
        full_text = extract_article_text(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            return full_text, None, None, None
        one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai(full_text, do_summary=do_summary)
        return one_line, summary, sentiment, full_text
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

def article_contains_exact_keyword(article, keywords):
    title = article.get("title", "")
    content = ""
    cache_key = article.get("link", "")
    summary_cache_key = None
    for key in st.session_state.keys():
        if key.startswith("summary_") and cache_key in key:
            summary_cache_key = key
            break
    if summary_cache_key and isinstance(st.session_state[summary_cache_key], tuple):
        _, _, _, content = st.session_state[summary_cache_key]
    for kw in keywords:
        if kw and (kw in title or (content and kw in content)):
            return True
    return False

search_clicked = False
if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

# --- ê²€ìƒ‰ íŠ¸ë¦¬ê±°: í‚¤ì›Œë“œ ì…ë ¥ or ì‚°ì—…ë³„ í•„í„° ì„ íƒ ì‹œ ---
if search_clicked or st.session_state.get("search_triggered"):
    # í‚¤ì›Œë“œ ì…ë ¥ì´ ìˆìœ¼ë©´ í•´ë‹¹ í‚¤ì›Œë“œë¡œ, ì—†ìœ¼ë©´ ì‚°ì—…ë³„ í•„í„° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
    if keywords_input:
        keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    else:
        keyword_list = st.session_state.get("industry_sub", [])
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    elif keyword_list:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=st.session_state.get("require_keyword_in_title", False))
    st.session_state.search_triggered = False

# --- ê¸°ì‚¬ í•„í„°ë§ í•¨ìˆ˜ ---
def article_passes_all_filters(article):
    filters = []
    # ê³µí†µ í•„í„° í•­ìƒ ì ìš©
    filters.append(ALL_COMMON_FILTER_KEYWORDS)
    # ì‚°ì—…ë³„ í•„í„° ì‚¬ìš© ì‹œ ì ìš©
    if st.session_state.get("use_industry_filter", False):
        filters.append(st.session_state.get("industry_sub", []))
    # ì œì™¸ í‚¤ì›Œë“œ
    if exclude_by_title_keywords(article.get('title', ''), EXCLUDE_TITLE_KEYWORDS):
        return False
    # í‚¤ì›Œë“œ ì •í™• í¬í•¨ ì˜µì…˜
    if st.session_state.get("require_exact_keyword_in_title_or_content", False):
        all_keywords = []
        if keywords_input:
            all_keywords.extend([k.strip() for k in keywords_input.split(",") if k.strip()])
        if st.session_state.get("industry_sub"):
            all_keywords.extend(st.session_state["industry_sub"])
        if not article_contains_exact_keyword(article, all_keywords):
            return False
    return or_keyword_filter(article, *filters)

def safe_title(val):
    if pd.isnull(val) or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
        return "ì œëª©ì—†ìŒ"
    return str(val)

def get_excel_download_with_favorite_and_excel_company_col(summary_data, favorite_categories, excel_company_categories):
    # ... (ìƒëµ, ê¸°ì¡´ ì½”ë“œ ë™ì¼) ...
    return output

def render_articles_with_single_summary_and_telegram(results, show_limit, show_sentiment_badge=True, enable_summary=True):
    # ... (ìƒëµ, ê¸°ì¡´ ì½”ë“œ ë™ì¼) ...
    pass

if st.session_state.search_results:
    filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        if filtered_articles:
            filtered_results[keyword] = filtered_articles
    render_articles_with_single_summary_and_telegram(
        filtered_results,
        st.session_state.show_limit,
        show_sentiment_badge=st.session_state.get("show_sentiment_badge", False),
        enable_summary=st.session_state.get("enable_summary", True)
    )
