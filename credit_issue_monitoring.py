import os
import streamlit as st
import pandas as pd
from io import BytesIO
import requests
import re
from datetime import datetime, date, timedelta
import telepot
from openai import OpenAI
import newspaper  # newspaper3k
import difflib

# --- CSS ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
[data-testid="column"] > div { gap: 0rem !important; }
.stMultiSelect [data-baseweb="tag"] { background-color: #ff5c5c !important; color: white !important; border: none !important; font-weight: bold; }
.sentiment-badge { display: inline-block; padding: 0.08em 0.6em; margin-left: 0.2em; border-radius: 0.8em; font-size: 0.85em; font-weight: bold; vertical-align: middle; }
.sentiment-positive { background: #2ecc40; color: #fff; }
.sentiment-negative { background: #ff4136; color: #fff; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em; padding: 1.1em 1.2em 1.2em 1.2em; box-shadow: 0 2px 8px 0 rgba(0,0,0,0.03); }
.flex-row-bottom { display: flex; align-items: flex-end; gap: 0.5rem; margin-bottom: 0.5rem; }
.flex-grow { flex: 1 1 0%; }
.flex-btn { min-width: 90px; }
</style>
""", unsafe_allow_html=True)

# ê²½ê³  ë©”ì‹œì§€(Warning, Exception ë“±) ì˜ì—­ì„ CSSë¡œ ìˆ¨ê¸°ê¸°
st.markdown("""
<style>
    .stAlert, .stException, .stWarning {
        display: none !important;
    }
    [data-testid="stNotification"] {
        display: none !important;
    }
</style>
""", unsafe_allow_html=True)

# --- ì œì™¸ í‚¤ì›Œë“œ ---
EXCLUDE_TITLE_KEYWORDS = [
    "ì•¼êµ¬", "ì¶•êµ¬", "ë°°êµ¬", "ë†êµ¬", "ê³¨í”„", "eìŠ¤í¬ì¸ ", "ì˜¬ë¦¼í”½", "ì›”ë“œì»µ", "Kë¦¬ê·¸", "í”„ë¡œì•¼êµ¬", "í”„ë¡œì¶•êµ¬", "í”„ë¡œë°°êµ¬", "í”„ë¡œë†êµ¬", "ìš°ìŠ¹", "ë¬´ìŠ¹ë¶€", "ê²½ê¸°", "íŒ¨ë°°", "ìŠ¤í¬ì¸ ", "ìŠ¤í°ì„œ",
    "ë¶€ê³ ", "ì¸ì‚¬", "ìŠ¹ì§„", "ì„ëª…", "ë°œë ¹", "ì¸ì‚¬ë°œë ¹", "ì¸ì‚¬ì´ë™",
    "ë¸Œëœë“œí‰íŒ", "ë¸Œëœë“œ í‰íŒ", "ë¸Œëœë“œ ìˆœìœ„", "ë¸Œëœë“œì§€ìˆ˜", "ì§€ì†ê°€ëŠ¥", "ESG", "ìŠ¤íƒ€íŠ¸ì—…",
    "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì£¼ê°€", "ì£¼ì‹", "ì¦ì‹œ", "ì‹œì„¸", "ë§ˆê°", "ì¥ì¤‘", "ì¥ë§ˆê°", "ê±°ë˜ëŸ‰", "ê±°ë˜ëŒ€ê¸ˆ", "ìƒí•œê°€", "í•˜í•œê°€",
    "ë´‰ì‚¬", "í›„ì›", "ê¸°ë¶€", "í˜œíƒ", "ë•¡ì²˜ë¦¬", "ì„¸ì¼", "ì´ë²¤íŠ¸"
]

def exclude_by_title_keywords(title, exclude_keywords):
    for word in exclude_keywords:
        if word in title:
            return True
    return False

# --- ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™” ---
if "favorite_keywords" not in st.session_state:
    st.session_state.favorite_keywords = set()
if "search_results" not in st.session_state:
    st.session_state.search_results = {}
if "show_limit" not in st.session_state:
    st.session_state.show_limit = {}
if "search_triggered" not in st.session_state:
    st.session_state.search_triggered = False
if "selected_articles" not in st.session_state:
    st.session_state.selected_articles = []
if "filtered_results" not in st.session_state:
    st.session_state.filtered_results = {}

# ìµœì´ˆ ì‹¤í–‰ ì‹œì—ë§Œ session_state ê°’ ì„¸íŒ…
if "end_date" not in st.session_state:
    st.session_state["end_date"] = date.today()
if "start_date" not in st.session_state:
    st.session_state["start_date"] = st.session_state["end_date"] - timedelta(days=7)
    
# --- ì¦ê²¨ì°¾ê¸° ì¹´í…Œê³ ë¦¬(ë³€ê²½ ê¸ˆì§€, UI ë¯¸ë…¸ì¶œ) ---
favorite_categories = {
    "êµ­/ê³µì±„": [],
    "ê³µê³µê¸°ê´€": [],
    "ë³´í—˜ì‚¬": ["í˜„ëŒ€í•´ìƒ", "ë†í˜‘ìƒëª…", "ë©”ë¦¬ì¸ í™”ì¬", "êµë³´ìƒëª…", "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…", "ì‹ í•œë¼ì´í”„", "í¥êµ­ìƒëª…", "ë™ì–‘ìƒëª…", "ë¯¸ë˜ì—ì…‹ìƒëª…"],
    "5ëŒ€ê¸ˆìœµì§€ì£¼": ["ì‹ í•œê¸ˆìœµ", "í•˜ë‚˜ê¸ˆìœµ", "KBê¸ˆìœµ", "ë†í˜‘ê¸ˆìœµ", "ìš°ë¦¬ê¸ˆìœµ"],
    "5ëŒ€ì‹œì¤‘ì€í–‰": ["ë†í˜‘ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "ìš°ë¦¬ì€í–‰", "í•˜ë‚˜ì€í–‰"],
    "ì¹´ë“œì‚¬": ["KBêµ­ë¯¼ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ì‹ í•œì¹´ë“œ", "ë¹„ì”¨ì¹´ë“œ", "ì‚¼ì„±ì¹´ë“œ"],
    "ìºí”¼íƒˆ": ["í•œêµ­ìºí”¼íƒˆ", "í˜„ëŒ€ìºí”¼íƒˆ"],
    "ì§€ì£¼ì‚¬": ["SKì´ë…¸ë² ì´ì…˜", "GSì—ë„ˆì§€", "SK", "GS"],
    "ì—ë„ˆì§€": ["SKê°€ìŠ¤", "GSì¹¼í…ìŠ¤", "S-Oil", "SKì—ë„ˆì§€", "SKì•¤ë¬´ë¸Œ", "ì½”ë¦¬ì•„ì—ë„ˆì§€í„°ë¯¸ë„"],
    "ë°œì „": ["GSíŒŒì›Œ", "GSEPS", "ì‚¼ì²œë¦¬"],
    "ìë™ì°¨": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜", "í•œì˜¨ì‹œìŠ¤í…œ", "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "í•œêµ­íƒ€ì´ì–´"],
    "ì „ê¸°/ì „ì": ["SKí•˜ì´ë‹‰ìŠ¤", "LGì´ë…¸í…", "LGì „ì", "LSì¼ë ‰íŠ¸ë¦­"],
    "ì†Œë¹„ì¬": ["ì´ë§ˆíŠ¸", "LF", "CJì œì¼ì œë‹¹", "SKë„¤íŠ¸ì›ìŠ¤", "CJëŒ€í•œí†µìš´"],
    "ë¹„ì² /ì² ê°•": ["í¬ìŠ¤ì½”", "í˜„ëŒ€ì œì² ", "ê³ ë ¤ì•„ì—°"],
    "ì„ìœ í™”í•™": ["LGí™”í•™", "SKì§€ì˜¤ì„¼íŠ¸ë¦­"],
    "ê±´ì„¤": ["í¬ìŠ¤ì½”ì´ì•¤ì”¨"],
    "íŠ¹ìˆ˜ì±„": ["ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬", "ê¸°ì—…ì€í–‰"]
}

# ---- ëŒ€ë¶„ë¥˜ë³„ ê¸°ì—…/ì´ìŠˆ ë¶„ë¦¬ ë° í†µí•© ----
# 1. ì€í–‰ ë° ê¸ˆìœµì§€ì£¼
industry_filter_categories = {}
industry_filter_categories["ì€í–‰ ë° ê¸ˆìœµì§€ì£¼"] = [
    "ê²½ì˜ì‹¤íƒœí‰ê°€", "BIS", "CET1", "ìë³¸ë¹„ìœ¨", "ìƒê°í˜• ì¡°ê±´ë¶€ìë³¸ì¦ê¶Œ", "ìë³¸í™•ì¶©", "ìë³¸ì—¬ë ¥", "ìë³¸ì ì •ì„±", "LCR",
    "ì¡°ë‹¬ê¸ˆë¦¬", "NIM", "ìˆœì´ìë§ˆì§„", "ê³ ì •ì´í•˜ì—¬ì‹ ë¹„ìœ¨", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ì—°ì²´ìœ¨", "ê°€ê³„ëŒ€ì¶œ", "ì·¨ì•½ì°¨ì£¼"
]
favorite_categories["ì€í–‰ ë° ê¸ˆìœµì§€ì£¼"] = favorite_categories["5ëŒ€ê¸ˆìœµì§€ì£¼"] + favorite_categories["5ëŒ€ì‹œì¤‘ì€í–‰"]

# 2. ì „ê¸°ì „ì
industry_filter_categories["ì „ê¸°ì „ì"] = [
    "CHIPS ë³´ì¡°ê¸ˆ", "ì¤‘êµ­", "DRAM", "HBM", "ê´‘í• ì†”ë£¨ì…˜", "ì•„ì´í°", "HVAC", "HVTR"
]
favorite_categories["ì „ê¸°ì „ì"] = favorite_categories["ì „ê¸°/ì „ì"]

# 3. ì² ê°•/ë¹„ì²  í†µí•©
industry_filter_categories["ì² ê°•/ë¹„ì² "] = [
    # ì² ê°• ì´ìŠˆ
    "ì² ê´‘ì„", "í›„íŒ", "ê°•íŒ", "ì² ê·¼", "ìŠ¤í”„ë ˆë“œ", "ì² ê°•", "ê°€ë™ë¥ ", "ì œì² ì†Œ", "ì…§ë‹¤ìš´", "ì¤‘êµ­ì‚° ì €ê°€",
    "ì¤‘êµ­ ìˆ˜ì¶œ ê°ì†Œ", "ê±´ì„¤ê²½ê¸°", "ì¡°ì„  ìˆ˜ìš”", "íŒŒì—…",
    # ë¹„ì²  ì´ìŠˆ
    "ì—°", "ì•„ì—°", "ë‹ˆì¼ˆ", "ì•ˆí‹°ëª¨ë‹ˆ", "ê²½ì˜ê¶Œ ë¶„ìŸ", "MBK", "ì˜í’"
]
favorite_categories["ì² ê°•/ë¹„ì² "] = favorite_categories["ë¹„ì² /ì² ê°•"]

# ê¸°ì¡´ "ì² ê°•", "ë¹„ì² ", "ì „ê¸°/ì „ì", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ë¹„ì² /ì² ê°•" ëŒ€ë¶„ë¥˜ëŠ” industry_filter_categoriesì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
# ë‚˜ë¨¸ì§€ ëŒ€ë¶„ë¥˜ëŠ” ê¸°ì¡´ëŒ€ë¡œ ì¶”ê°€
industry_filter_categories.update({
    "ë³´í—˜ì‚¬": [
        "ë³´ì¥ì„±ë³´í—˜", "ì €ì¶•ì„±ë³´í—˜", "ë³€ì•¡ë³´í—˜", "í‡´ì§ì—°ê¸ˆ", "ì¼ë°˜ë³´í—˜", "ìë™ì°¨ë³´í—˜", "ALM", "ì§€ê¸‰ì—¬ë ¥ë¹„ìœ¨", "K-ICS",
        "ë³´í—˜ìˆ˜ìµì„±", "ë³´í—˜ì†ìµ", "ìˆ˜ì…ë³´í—˜ë£Œ", "CSM", "ìƒê°", "íˆ¬ìì†ìµ", "ìš´ìš©ì„±ê³¼", "IFRS4", "IFRS17", "ë³´í—˜ë¶€ì±„",
        "ì¥ê¸°ì„ ë„ê¸ˆë¦¬", "ìµœì¢…ê´€ì°°ë§Œê¸°", "ìœ ë™ì„± í”„ë¦¬ë¯¸ì—„", "ì‹ ì¢…ìë³¸ì¦ê¶Œ", "í›„ìˆœìœ„ì±„", "ìœ„í—˜ìì‚°ë¹„ì¤‘", "ê°€ì¤‘ë¶€ì‹¤ìì‚°ë¹„ìœ¨"
    ],
    "ì¹´ë“œì‚¬": [
        "ë¯¼ê°„ì†Œë¹„ì§€í‘œ", "ëŒ€ì†ì¤€ë¹„ê¸ˆ", "ê°€ê³„ë¶€ì±„", "ì—°ì²´ìœ¨", "ê°€ë§¹ì ì¹´ë“œìˆ˜ìˆ˜ë£Œ", "ëŒ€ì¶œì„±ìì‚°", "ì‹ ìš©íŒë§¤ìì‚°", "ê³ ì •ì´í•˜ì—¬ì‹ ", "ë ˆë²„ë¦¬ì§€ë°°ìœ¨",
        "ê±´ì „ì„±", "ì¼€ì´ë±…í¬", "ì´íƒˆ"
    ],
    "ìºí”¼íƒˆ": [
        "ì¶©ë‹¹ê¸ˆì»¤ë²„ë¦¬ì§€ë¹„ìœ¨", "ê³ ì •ì´í•˜ì—¬ì‹ ", "PFêµ¬ì¡°ì¡°ì •", "ë¦¬ìŠ¤ìì‚°", "ì†ì‹¤í¡ìˆ˜ëŠ¥ë ¥", "ë¶€ë™ì‚°PFì—°ì²´ì±„ê¶Œ", "ìì‚°í¬íŠ¸í´ë¦¬ì˜¤", "ê±´ì „ì„±",
        "ì¡°ì •ì´ìì‚°ìˆ˜ìµë¥ ", "êµ°ì¸ê³µì œíšŒ"
    ],
    "ì§€ì£¼ì‚¬": [
        "SKì§€ì˜¤ì„¼íŠ¸ë¦­", "SKì—ë„ˆì§€", "SKì—”ë¬´ë¸Œ", "SKì¸ì²œì„ìœ í™”í•™", "GSì¹¼í…ìŠ¤", "GSíŒŒì›Œ", "SKì´ë…¸ë² ì´ì…˜", "SKí…”ë ˆì½¤", "SKì˜¨",
        "GSì—ë„ˆì§€", "GSë¦¬í…Œì¼", "GS E&C", "2ì°¨ì „ì§€", "ì„ìœ í™”í•™", "ìœ¤í™œìœ ", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ì •ìœ ", "ì´ë™í†µì‹ "
    ],
    "ì—ë„ˆì§€": [
        "ì •ìœ ", "ìœ ê°€", "ì •ì œë§ˆì§„", "ìŠ¤í”„ë ˆë“œ", "ê°€ë™ë¥ ", "ì¬ê³  ì†ì‹¤", "ì¤‘êµ­ ìˆ˜ìš”", "IMO ê·œì œ", "ì €ìœ í™© ì—°ë£Œ", "LNG",
        "í„°ë¯¸ë„", "ìœ¤í™œìœ "
    ],
    "ë°œì „": [
        "LNG", "ì²œì—°ê°€ìŠ¤", "ìœ ê°€", "SMP", "REC", "ê³„í†µì‹œì¥", "íƒ„ì†Œì„¸", "íƒ„ì†Œë°°ì¶œê¶Œ", "ì „ë ¥ì‹œì¥ ê°œí¸", "ì „ë ¥ ììœ¨í™”",
        "ê°€ë™ë¥ ", "ë„ì‹œê°€ìŠ¤"
    ],
    "ìë™ì°¨": [
        "AMPC ë³´ì¡°ê¸ˆ", "IRA ì¸ì„¼í‹°ë¸Œ", "ì¤‘êµ­ ë°°í„°ë¦¬", "EV ìˆ˜ìš”", "ì „ê¸°ì°¨", "ESSìˆ˜ìš”", "ë¦¬íŠ¬", "íƒ€ì´ì–´"
    ],
    "ì†Œë¹„ì¬": [
        "ë‚´ìˆ˜ë¶€ì§„", "ì‹œì¥ì§€ë°°ë ¥", "SKí…”ë ˆì½¤", "SKë§¤ì§", "CLS", "HMR", "ë¼ì´ì‹ ", "ì•„ë¯¸ë…¸ì‚°", "ìŠˆì™„ìŠ¤ì»´í¼ë‹ˆ",
        "ì˜ë¥˜", "ì‹ ì„¸ê³„", "ëŒ€í˜•ë§ˆíŠ¸ ì˜ë¬´íœ´ì—…", "Gë§ˆì¼“", "Wì»¨ì…‰", "ìŠ¤íƒ€í•„ë“œ"
    ],
    "ì„ìœ í™”í•™": [
        "ì„ìœ í™”í•™", "ì„í™”", "ìœ ê°€", "ì¦ì„¤", "ìŠ¤í”„ë ˆë“œ", "ê°€ë™ë¥ ", "PX", "ë²¤ì  ", "ì¤‘êµ­ ì¦ì„¤", "ì¤‘ë™ COTC",
        "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ë¦¬íŠ¬", "IRA", "AMPC"
    ],
    "ê±´ì„¤": [
        "ì² ê·¼ ê°€ê²©", "ì‹œë©˜íŠ¸ ê°€ê²©", "ê³µì‚¬ë¹„", "SOC ì˜ˆì‚°", "ë„ì‹œì •ë¹„ ì§€ì›", "ìš°ë°œì±„ë¬´", "ìˆ˜ì£¼", "ì£¼ê°„ì‚¬", "ì‚¬ê³ ",
        "ì‹œê³µëŠ¥ë ¥ìˆœìœ„", "ë¯¸ë¶„ì–‘", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ"
    ],
    "íŠ¹ìˆ˜ì±„": [
        "ìë³¸í™•ì¶©", "HUG", "ì „ì„¸ì‚¬ê¸°", "ë³´ì¦ì‚¬ê³ ", "ë³´ì¦ë£Œìœ¨", "íšŒìˆ˜ìœ¨", "ë³´ì¦ì”ì•¡", "ëŒ€ìœ„ë³€ì œì•¡",
        "ì¤‘ì†Œê¸°ì—…ëŒ€ì¶œ", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ë¶ˆë²•", "êµ¬ì†"
    ]
})

# --- ê³µí†µ í•„í„° ì˜µì…˜(ëŒ€ë¶„ë¥˜/ì†Œë¶„ë¥˜ ì—†ì´ ëª¨ë‘ ì ìš©) ---
common_filter_categories = {
    "ì‹ ìš©/ë“±ê¸‰": [
        "ì‹ ìš©ë“±ê¸‰", "ë“±ê¸‰ì „ë§", "í•˜ë½", "ê°•ë“±", "í•˜í–¥", "ìƒí–¥", "ë””í´íŠ¸", "ë¶€ì‹¤", "ë¶€ë„", "ë¯¸ì§€ê¸‰", "ìˆ˜ìš” ë¯¸ë‹¬", "ë¯¸ë§¤ê°", "ì œë„ ê°œí¸", "EOD"
    ],
    "ìˆ˜ìš”/ê³µê¸‰": [
        "ìˆ˜ìš”", "ê³µê¸‰", "ìˆ˜ê¸‰", "ë‘”í™”", "ìœ„ì¶•", "ì„±ì¥", "ê¸‰ë“±", "ê¸‰ë½", "ìƒìŠ¹", "í•˜ë½", "ë¶€ì§„", "ì‹¬í™”"
    ],
    "ì‹¤ì /ì¬ë¬´": [
        "ì‹¤ì ", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ì ì", "ì†ì‹¤", "ë¹„ìš©", "ë¶€ì±„ë¹„ìœ¨", "ì´ìë³´ìƒë°°ìœ¨"
    ],
    "ìê¸ˆ/ì¡°ë‹¬": [
        "ì°¨ì…", "ì¡°ë‹¬", "ì„¤ë¹„íˆ¬ì", "íšŒì‚¬ì±„", "ë°œí–‰", "ì¸ìˆ˜", "ë§¤ê°"
    ],
    "êµ¬ì¡°/ì¡°ì •": [
        "M&A", "í•©ë³‘", "ê³„ì—´ ë¶„ë¦¬", "êµ¬ì¡°ì¡°ì •", "ë‹¤ê°í™”", "êµ¬ì¡° ì¬í¸"
    ],
    "ê±°ì‹œ/ì •ì±…": [
        "ê¸ˆë¦¬", "í™˜ìœ¨", "ê´€ì„¸", "ë¬´ì—­ì œì¬", "ë³´ì¡°ê¸ˆ", "ì„¸ì•¡ ê³µì œ", "ê²½ìŸ"
    ],
    "ì§€ë°°êµ¬ì¡°/ë²•": [
        "íš¡ë ¹", "ë°°ì„", "ê³µì •ê±°ë˜", "ì˜¤ë„ˆë¦¬ìŠ¤í¬", "ëŒ€ì£¼ì£¼", "ì§€ë°°êµ¬ì¡°"
    ]
}
ALL_COMMON_FILTER_KEYWORDS = []
for keywords in common_filter_categories.values():
    ALL_COMMON_FILTER_KEYWORDS.extend(keywords)

# --- ì‚°ì—…ë³„ í•„í„° ì˜µì…˜ + ì¦ê²¨ì°¾ê¸° ê¸°ì—…ëª… í¬í•¨ ---
industry_filter_categories = {
    "ì€í–‰ ë° ê¸ˆìœµì§€ì£¼": [
        "ê²½ì˜ì‹¤íƒœí‰ê°€", "BIS", "CET1", "ìë³¸ë¹„ìœ¨", "ìƒê°í˜• ì¡°ê±´ë¶€ìë³¸ì¦ê¶Œ", "ìë³¸í™•ì¶©", "ìë³¸ì—¬ë ¥", "ìë³¸ì ì •ì„±", "LCR",
        "ì¡°ë‹¬ê¸ˆë¦¬", "NIM", "ìˆœì´ìë§ˆì§„", "ê³ ì •ì´í•˜ì—¬ì‹ ë¹„ìœ¨", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ì—°ì²´ìœ¨", "ê°€ê³„ëŒ€ì¶œ", "ì·¨ì•½ì°¨ì£¼"
    ] + favorite_categories["5ëŒ€ê¸ˆìœµì§€ì£¼"] + favorite_categories["5ëŒ€ì‹œì¤‘ì€í–‰"],
    "ë³´í—˜ì‚¬": [
        "ë³´ì¥ì„±ë³´í—˜", "ì €ì¶•ì„±ë³´í—˜", "ë³€ì•¡ë³´í—˜", "í‡´ì§ì—°ê¸ˆ", "ì¼ë°˜ë³´í—˜", "ìë™ì°¨ë³´í—˜", "ALM", "ì§€ê¸‰ì—¬ë ¥ë¹„ìœ¨", "K-ICS",
        "ë³´í—˜ìˆ˜ìµì„±", "ë³´í—˜ì†ìµ", "ìˆ˜ì…ë³´í—˜ë£Œ", "CSM", "ìƒê°", "íˆ¬ìì†ìµ", "ìš´ìš©ì„±ê³¼", "IFRS4", "IFRS17", "ë³´í—˜ë¶€ì±„",
        "ì¥ê¸°ì„ ë„ê¸ˆë¦¬", "ìµœì¢…ê´€ì°°ë§Œê¸°", "ìœ ë™ì„± í”„ë¦¬ë¯¸ì—„", "ì‹ ì¢…ìë³¸ì¦ê¶Œ", "í›„ìˆœìœ„ì±„", "ìœ„í—˜ìì‚°ë¹„ì¤‘", "ê°€ì¤‘ë¶€ì‹¤ìì‚°ë¹„ìœ¨"
    ] + favorite_categories["ë³´í—˜ì‚¬"],
    "ì¹´ë“œì‚¬": [
        "ë¯¼ê°„ì†Œë¹„ì§€í‘œ", "ëŒ€ì†ì¤€ë¹„ê¸ˆ", "ê°€ê³„ë¶€ì±„", "ì—°ì²´ìœ¨", "ê°€ë§¹ì ì¹´ë“œìˆ˜ìˆ˜ë£Œ", "ëŒ€ì¶œì„±ìì‚°", "ì‹ ìš©íŒë§¤ìì‚°", "ê³ ì •ì´í•˜ì—¬ì‹ ", "ë ˆë²„ë¦¬ì§€ë°°ìœ¨",
        "ê±´ì „ì„±", "ì¼€ì´ë±…í¬", "ì´íƒˆ"
    ] + favorite_categories["ì¹´ë“œì‚¬"],
    "ìºí”¼íƒˆ": [
        "ì¶©ë‹¹ê¸ˆì»¤ë²„ë¦¬ì§€ë¹„ìœ¨", "ê³ ì •ì´í•˜ì—¬ì‹ ", "PFêµ¬ì¡°ì¡°ì •", "ë¦¬ìŠ¤ìì‚°", "ì†ì‹¤í¡ìˆ˜ëŠ¥ë ¥", "ë¶€ë™ì‚°PFì—°ì²´ì±„ê¶Œ", "ìì‚°í¬íŠ¸í´ë¦¬ì˜¤", "ê±´ì „ì„±",
        "ì¡°ì •ì´ìì‚°ìˆ˜ìµë¥ ", "êµ°ì¸ê³µì œíšŒ"
    ] + favorite_categories["ìºí”¼íƒˆ"],
    "ì§€ì£¼ì‚¬": [
        "SKì§€ì˜¤ì„¼íŠ¸ë¦­", "SKì—ë„ˆì§€", "SKì—”ë¬´ë¸Œ", "SKì¸ì²œì„ìœ í™”í•™", "GSì¹¼í…ìŠ¤", "GSíŒŒì›Œ", "SKì´ë…¸ë² ì´ì…˜", "SKí…”ë ˆì½¤", "SKì˜¨",
        "GSì—ë„ˆì§€", "GSë¦¬í…Œì¼", "GS E&C", "2ì°¨ì „ì§€", "ì„ìœ í™”í•™", "ìœ¤í™œìœ ", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ì •ìœ ", "ì´ë™í†µì‹ "
    ] + favorite_categories["ì§€ì£¼ì‚¬"],
    "ì—ë„ˆì§€": [
        "ì •ìœ ", "ìœ ê°€", "ì •ì œë§ˆì§„", "ìŠ¤í”„ë ˆë“œ", "ê°€ë™ë¥ ", "ì¬ê³  ì†ì‹¤", "ì¤‘êµ­ ìˆ˜ìš”", "IMO ê·œì œ", "ì €ìœ í™© ì—°ë£Œ", "LNG",
        "í„°ë¯¸ë„", "ìœ¤í™œìœ "
    ] + favorite_categories["ì—ë„ˆì§€"],
    "ë°œì „": [
        "LNG", "ì²œì—°ê°€ìŠ¤", "ìœ ê°€", "SMP", "REC", "ê³„í†µì‹œì¥", "íƒ„ì†Œì„¸", "íƒ„ì†Œë°°ì¶œê¶Œ", "ì „ë ¥ì‹œì¥ ê°œí¸", "ì „ë ¥ ììœ¨í™”",
        "ê°€ë™ë¥ ", "ë„ì‹œê°€ìŠ¤"
    ] + favorite_categories["ë°œì „"],
    "ìë™ì°¨": [
        "AMPC ë³´ì¡°ê¸ˆ", "IRA ì¸ì„¼í‹°ë¸Œ", "ì¤‘êµ­ ë°°í„°ë¦¬", "EV ìˆ˜ìš”", "ì „ê¸°ì°¨", "ESSìˆ˜ìš”", "ë¦¬íŠ¬", "íƒ€ì´ì–´"
    ] + favorite_categories["ìë™ì°¨"],
    "ì „ê¸°ì „ì": [
        "CHIPS ë³´ì¡°ê¸ˆ", "ì¤‘êµ­", "DRAM", "HBM", "ê´‘í• ì†”ë£¨ì…˜", "ì•„ì´í°", "HVAC", "HVTR"
    ] + favorite_categories["ì „ê¸°/ì „ì"],
    "ì² ê°•": [
        "ì² ê´‘ì„", "í›„íŒ", "ê°•íŒ", "ì² ê·¼", "ìŠ¤í”„ë ˆë“œ", "ì² ê°•", "ê°€ë™ë¥ ", "ì œì² ì†Œ", "ì…§ë‹¤ìš´", "ì¤‘êµ­ì‚° ì €ê°€",
        "ì¤‘êµ­ ìˆ˜ì¶œ ê°ì†Œ", "ê±´ì„¤ê²½ê¸°", "ì¡°ì„  ìˆ˜ìš”", "íŒŒì—…"
    ] + favorite_categories["ë¹„ì² /ì² ê°•"],
    "ë¹„ì² ": [
        "ì—°", "ì•„ì—°", "ë‹ˆì¼ˆ", "ì•ˆí‹°ëª¨ë‹ˆ", "ê²½ì˜ê¶Œ ë¶„ìŸ", "MBK", "ì˜í’"
    ],
    "ì†Œë¹„ì¬": [
        "ë‚´ìˆ˜ë¶€ì§„", "ì‹œì¥ì§€ë°°ë ¥", "SKí…”ë ˆì½¤", "SKë§¤ì§", "CLS", "HMR", "ë¼ì´ì‹ ", "ì•„ë¯¸ë…¸ì‚°", "ìŠˆì™„ìŠ¤ì»´í¼ë‹ˆ",
        "ì˜ë¥˜", "ì‹ ì„¸ê³„", "ëŒ€í˜•ë§ˆíŠ¸ ì˜ë¬´íœ´ì—…", "Gë§ˆì¼“", "Wì»¨ì…‰", "ìŠ¤íƒ€í•„ë“œ"
    ] + favorite_categories["ì†Œë¹„ì¬"],
    "ì„ìœ í™”í•™": [
        "ì„ìœ í™”í•™", "ì„í™”", "ìœ ê°€", "ì¦ì„¤", "ìŠ¤í”„ë ˆë“œ", "ê°€ë™ë¥ ", "PX", "ë²¤ì  ", "ì¤‘êµ­ ì¦ì„¤", "ì¤‘ë™ COTC",
        "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ë¦¬íŠ¬", "IRA", "AMPC"
    ] + favorite_categories["ì„ìœ í™”í•™"],
    "ê±´ì„¤": [
        "ì² ê·¼ ê°€ê²©", "ì‹œë©˜íŠ¸ ê°€ê²©", "ê³µì‚¬ë¹„", "SOC ì˜ˆì‚°", "ë„ì‹œì •ë¹„ ì§€ì›", "ìš°ë°œì±„ë¬´", "ìˆ˜ì£¼", "ì£¼ê°„ì‚¬", "ì‚¬ê³ ",
        "ì‹œê³µëŠ¥ë ¥ìˆœìœ„", "ë¯¸ë¶„ì–‘", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ"
    ] + favorite_categories["ê±´ì„¤"],
    "íŠ¹ìˆ˜ì±„": [
        "ìë³¸í™•ì¶©", "HUG", "ì „ì„¸ì‚¬ê¸°", "ë³´ì¦ì‚¬ê³ ", "ë³´ì¦ë£Œìœ¨", "íšŒìˆ˜ìœ¨", "ë³´ì¦ì”ì•¡", "ëŒ€ìœ„ë³€ì œì•¡",
        "ì¤‘ì†Œê¸°ì—…ëŒ€ì¶œ", "ëŒ€ì†ì¶©ë‹¹ê¸ˆ", "ë¶€ì‹¤ì±„ê¶Œ", "ë¶ˆë²•", "êµ¬ì†"
    ] + favorite_categories["íŠ¹ìˆ˜ì±„"]
}

KOREAN_STOPWORDS = {
    'ì˜', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë¡œ', 'ë°', 'í•œ', 'í•˜ë‹¤', 'ë˜ë‹¤',
    'â€¦', 'â€œ', 'â€', 'â€˜', 'â€™', 'ë“±', 'ë°', 'ê·¸', 'ì €', 'ë”', 'ë˜', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë¡œ', 'ì—ì„œ', 'ê¹Œì§€', 'ë¶€í„°'
}
ENGLISH_STOPWORDS = {
    "the", "and", "is", "in", "to", "of", "a", "on", "for", "with", "as", "by", "at", "an", "be", "from", "it", "that",
    "this", "are", "was", "but", "or", "not", "has", "have", "had", "will", "would", "can", "could", "should"
}

def extract_keywords(text):
    if re.search(r"[ê°€-í£]", text):
        words = re.findall(r"[ê°€-í£]{2,}", text)
        keywords = [w for w in words if w not in KOREAN_STOPWORDS]
        return set(keywords)
    else:
        words = re.findall(r'\b[a-zA-Z]{2,}\b', text.lower())
        keywords = [w for w in words if w not in ENGLISH_STOPWORDS]
        return set(keywords)

def remove_duplicate_articles_by_title_and_keywords(articles, title_threshold=0.75, keyword_threshold=0.6):
    unique_articles = []
    seen_titles = set()
    seen_keywords_hash = set()
    for article in articles:
        title = article.get("title", "")
        full_text = article.get("title", "") + " " + article.get("description", "")
        keywords = extract_keywords(full_text)
        title_key = title.strip().lower()
        kw_hash = hash(frozenset(keywords))
        if title_key in seen_titles or kw_hash in seen_keywords_hash:
            continue
        unique_articles.append(article)
        seen_titles.add(title_key)
        seen_keywords_hash.add(kw_hash)
    return unique_articles

# UI ì‹œì‘
st.set_page_config(layout="wide")
col_title, col_option1, col_option2 = st.columns([0.6, 0.2, 0.2])
with col_title:
    st.markdown("<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>ğŸ“Š Credit Issue Monitoring</h1>", unsafe_allow_html=True)
with col_option1:
    show_sentiment_badge = st.checkbox("ê¸°ì‚¬ëª©ë¡ì— ê°ì„±ë¶„ì„ ë°°ì§€ í‘œì‹œ", value=False, key="show_sentiment_badge")
with col_option2:
    enable_summary = st.checkbox("ìš”ì•½ ê¸°ëŠ¥ ì ìš©", value=False, key="enable_summary")

# 1. í‚¤ì›Œë“œ ì…ë ¥/ê²€ìƒ‰ ë²„íŠ¼ (í•œ ì¤„, ë²„íŠ¼ ì˜¤ë¥¸ìª½)
col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    keywords_input = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ì‚¼ì„±, í•œí™”)", value="", key="keyword_input", label_visibility="visible")
with col_kw_btn:
    search_clicked = st.button("ê²€ìƒ‰", key="search_btn", help="í‚¤ì›Œë“œë¡œ ê²€ìƒ‰", use_container_width=True)

# 2. ì‚°ì—…ë³„ ê²€ìƒ‰ (í‚¤ì›Œë“œ ê²€ìƒ‰ë€ ë°”ë¡œ ì•„ë˜, ëŒ€ë¶„ë¥˜-ê¸°ì—…-ì´ìŠˆ 3ë‹¨ê³„)
st.markdown("### ğŸ­ ì‚°ì—…ë³„ ê²€ìƒ‰")
col_major, col_company, col_issue, col_btn = st.columns([0.25, 0.25, 0.30, 0.20])

with col_major:
    selected_industry = st.selectbox(
        "ëŒ€ë¶„ë¥˜(ì‚°ì—…)",
        list(industry_filter_categories.keys()),
        key="industry_major"
    )

# í•´ë‹¹ ì‚°ì—…êµ°ì˜ ê¸°ì—…/ì´ìŠˆ ë¶„ë¦¬
industry_companies = favorite_categories.get(selected_industry, [])
industry_issues = [k for k in industry_filter_categories[selected_industry] if k not in industry_companies]

with col_company:
    # ëŒ€ë¶„ë¥˜ ì„ íƒ ì‹œ ê¸°ì—… ìë™ ì „ì²´ ì„ íƒ
    selected_companies = st.multiselect(
        "ê¸°ì—…",
        industry_companies,
        default=industry_companies,
        key="industry_companies"
    )

with col_issue:
    selected_issues = st.multiselect(
        "ì†Œë¶„ë¥˜(ì´ìŠˆ)",
        industry_issues,
        default=industry_issues,
        key="industry_issues"
    )

with col_btn:
    industry_search_clicked = st.button("ê²€ìƒ‰", key="industry_search_btn", use_container_width=True)

# 3. ë‚ ì§œ ìœ„ì ¯
def on_date_change():
    filter_articles_by_date()

date_col1, date_col2 = st.columns([1, 1])
with date_col2:
    st.date_input(
        "ì¢…ë£Œì¼",
        value=st.session_state["end_date"],
        key="end_date",
        on_change=on_date_change
    )
with date_col1:
    st.date_input(
        "ì‹œì‘ì¼",
        value=st.session_state["start_date"],
        key="start_date",
        on_change=on_date_change
    )
    
# --- ê³µí†µ í•„í„° ì˜µì…˜ (í•­ìƒ ì ìš©, ì „ì²´ í‚¤ì›Œë“œ ê°€ì‹œì ìœ¼ë¡œ í‘œì‹œ) ---
with st.expander("ğŸ§© ê³µí†µ í•„í„° ì˜µì…˜ (í•„í„°ë³„ ì ìš©/í•´ì œ ê°€ëŠ¥)"):
    common_filter_active = {}
    for major, subs in common_filter_categories.items():
        active = st.checkbox(f"{major} í•„í„° ì ìš©", value=True, key=f"common_filter_{major}")
        common_filter_active[major] = active
        st.markdown(f"- {', '.join(subs)}")

# í•„í„°ë§ ì‹œ ì ìš©í•  í‚¤ì›Œë“œë§Œ ëª¨ìŒ
active_common_keywords = []
for major, active in common_filter_active.items():
    if active:
        active_common_keywords.extend(common_filter_categories[major])

# --- í‚¤ì›Œë“œ í•„í„° ì˜µì…˜ (í•˜ë‹¨ìœ¼ë¡œ ì´ë™) ---
with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜"):
    require_keyword_in_title = st.checkbox("ê¸°ì‚¬ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ë³´ê¸°", value=False, key="require_keyword_in_title")
    require_exact_keyword_in_title_or_content = st.checkbox("í‚¤ì›Œë“œê°€ ì˜¨ì „íˆ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë³´ê¸°", value=False, key="require_exact_keyword_in_title_or_content")

# --- ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ (ìºì‹±) ---
@st.cache_data(show_spinner=False)
def extract_article_text_cached(url):
    try:
        article = newspaper.Article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

# --- OpenAI ìš”ì•½/ê°ì„±ë¶„ì„ í•¨ìˆ˜ (ìºì‹±) ---
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

@st.cache_data(show_spinner=False)
def summarize_and_sentiment_with_openai_cached(text, title=None, do_summary=True):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None, None

    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜¤ë¥˜ì¼ ë•Œ ìš”ì•½ ì‹œë„ ê¸ˆì§€
    if not text or "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜" in text or len(text.strip()) < 50:
        return "ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ìš”ì•½ ë¶ˆê°€", None, None, None

    lang = detect_lang(text)
    title = title or ""
    if lang == "ko":
        prompt = (
            f"ì•„ë˜ ê¸°ì‚¬ ì œëª©, ë³¸ë¬¸, ê·¸ë¦¬ê³  ê²€ìƒ‰ í‚¤ì›Œë“œ '{title}'ë¥¼ ì°¸ê³ í•´, ë°˜ë“œì‹œ í•œ ì¤„ ìš”ì•½ì— '{title}'ê°€ í¬í•¨ë˜ë„ë¡ í•´ì¤˜. "
            "ë§Œì•½ í‚¤ì›Œë“œì™€ ê´€ë ¨ ì—†ëŠ” ê¸°ì‚¬ë¼ë©´ 'í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë‚´ìš©ì´ ê¸°ì‚¬ì— ì—†ìŒ'ì´ë¼ê³  ë‹µí•´ì¤˜.\n"
            "- [í•œ ì¤„ ìš”ì•½]: ê¸°ì‚¬ ì œëª©, ë³¸ë¬¸, í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë°˜ë“œì‹œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\n"
            "- [ê°ì„±]: ê¸°ì‚¬ ì „ì²´ì˜ ê°ì •ì„ ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì¤˜. ì¤‘ë¦½ì€ ì ˆëŒ€ ë‹µí•˜ì§€ ë§ˆ. íŒŒì‚°, ìê¸ˆë‚œ ë“± ë¶€ì •ì  ì‚¬ê±´ì´ ì¤‘ì‹¬ì´ë©´ ë°˜ë“œì‹œ 'ë¶€ì •'ìœ¼ë¡œ ë‹µí•´ì¤˜.\n\n"
            "ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n"
            "[í•œ ì¤„ ìš”ì•½]: (ì—¬ê¸°ì— í•œ ì¤„ ìš”ì•½)\n"
            "[ê°ì„±]: (ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜ë§Œ)\n\n"
            f"[ê²€ìƒ‰ í‚¤ì›Œë“œ]\n{title}\n[ê¸°ì‚¬ ì œëª©]\n{title}\n[ê¸°ì‚¬ ë³¸ë¬¸]\n{text}"
        )
    else:
        prompt = (
        f"Summarize the following article in one sentence, and make sure the summary includes the keyword '{title}'. "
        "If the keyword is not relevant to the article, answer: 'No content related to the keyword.'\n"
        "- [One-line Summary]: Summarize with the keyword included.\n"
        "- [Sentiment]: positive or negative only.\n\n"
        "Respond in this format:\n"
        "[One-line Summary]: (your one-line summary)\n"
        "[Sentiment]: (positive/negative only)\n\n"
        f"[KEYWORD]\n{title}\n[TITLE]\n{title}\n[ARTICLE]\n{text}"
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*(.+)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if (do_summary and m1) else ""
    summary = ""  # ìƒì„¸ ìš”ì•½ì€ ìƒëµ
    sentiment = m3.group(1).strip() if m3 else ""
    # í›„ì²˜ë¦¬: ì¤‘ë¦½ ë“± ë“¤ì–´ì˜¤ë©´ ë¶€ì •ìœ¼ë¡œ ê°•ì œ
    if sentiment.lower() in ['neutral', 'ì¤‘ë¦½', '']:
        sentiment = 'ë¶€ì •' if lang == "ko" else 'negative'
    if lang == "en":
        sentiment = 'ê¸ì •' if sentiment.lower() == 'positive' else 'ë¶€ì •'
    return one_line, summary, sentiment, text

def summarize_article_from_url(article_url, title, do_summary=True):
    try:
        full_text = extract_article_text_cached(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            return full_text, None, None, None
        one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai_cached(full_text, title=title, do_summary=do_summary)
        return one_line, summary, sentiment, full_text
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None

NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(re.sub("<.*?>", "", title), EXCLUDE_TITLE_KEYWORDS):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "description": re.sub("<.*?>", "", desc),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
        if len(items) < 100:
            break
    return articles[:limit]

def fetch_gnews_news(query, start_date=None, end_date=None, limit=100, require_keyword_in_title=False):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(title, EXCLUDE_TITLE_KEYWORDS):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "description": desc,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        else:
            articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        articles = remove_duplicate_articles_by_title_and_keywords(articles, title_threshold=0.75, keyword_threshold=0.6)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

def detect_lang_from_title(title):
    return "ko" if re.search(r"[ê°€-í£]", title) else "en"

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

def article_contains_exact_keyword(article, keywords):
    title = article.get("title", "")
    content = ""
    cache_key = article.get("link", "")
    summary_cache_key = None
    for key in st.session_state.keys():
        if key.startswith("summary_") and cache_key in key:
            summary_cache_key = key
            break
    if summary_cache_key and isinstance(st.session_state[summary_cache_key], tuple):
        _, _, _, content = st.session_state[summary_cache_key]
    for kw in keywords:
        if kw and (kw in title or (content and kw in content)):
            return True
    return False

search_clicked = False
if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

if search_clicked or st.session_state.get("search_triggered"):
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, st.session_state["start_date"], st.session_state["end_date"], require_keyword_in_title=st.session_state.get("require_keyword_in_title", False))
    st.session_state.search_triggered = False

# ì‚°ì—…ë³„ ê²€ìƒ‰ ë²„íŠ¼ ë™ì‘ (ëŒ€ë¶„ë¥˜-ê¸°ì—…-ì´ìŠˆ êµ¬ì¡°)
if industry_search_clicked and selected_companies:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        process_keywords(
            selected_companies,
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_keyword_in_title", False)
        )
    # ì´ìŠˆëŠ” í›„ì²˜ë¦¬ í•„í„°ì—ì„œ ì ìš©

def article_passes_all_filters(article):
    filters = []
    filters.append(ALL_COMMON_FILTER_KEYWORDS)
    # ì‚°ì—…ë³„ ê²€ìƒ‰ì—ì„œ ì´ìŠˆ(OR) í•„í„° ì ìš©
    industry_issues_filter = st.session_state.get("industry_issues", [])
    if industry_issues_filter:
        filters.append(industry_issues_filter)
    # í™œì„±í™”ëœ ê³µí†µ í•„í„°ë§Œ ì ìš©
    if active_common_keywords:
        filters.append(active_common_keywords)
    if exclude_by_title_keywords(article.get('title', ''), EXCLUDE_TITLE_KEYWORDS):
        return False
    if st.session_state.get("require_exact_keyword_in_title_or_content", False):
        all_keywords = []
        if keywords_input:
            all_keywords.extend([k.strip() for k in keywords_input.split(",") if k.strip()])
        if not article_contains_exact_keyword(article, all_keywords):
            return False
    return or_keyword_filter(article, *filters)

def safe_title(val):
    if pd.isnull(val) or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
        return "ì œëª©ì—†ìŒ"
    return str(val)

def get_excel_download_with_favorite_and_excel_company_col(summary_data, favorite_categories, excel_company_categories):
    company_order = []
    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        company_order.extend(favorite_categories.get(cat, []))

    excel_company_order = []
    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        excel_company_order.extend(excel_company_categories.get(cat, []))

    df_articles = pd.DataFrame(summary_data)
    result_rows = []
    for idx, company in enumerate(company_order):
        excel_company_name = excel_company_order[idx] if idx < len(excel_company_order) else ""

        comp_articles = df_articles[df_articles["í‚¤ì›Œë“œ"] == company]
        pos_news = comp_articles[comp_articles["ê°ì„±"] == "ê¸ì •"].sort_values(by="ë‚ ì§œ", ascending=False)
        neg_news = comp_articles[comp_articles["ê°ì„±"] == "ë¶€ì •"].sort_values(by="ë‚ ì§œ", ascending=False)

        if not pos_news.empty:
            pos_date = pos_news.iloc[0]["ë‚ ì§œ"]
            pos_title = pos_news.iloc[0]["ê¸°ì‚¬ì œëª©"]
            pos_link = pos_news.iloc[0]["ë§í¬"]
            pos_display = f'({pos_date}) {pos_title}'
            pos_hyperlink = f'=HYPERLINK("{pos_link}", "{pos_display}")'
        else:
            pos_hyperlink = ""

        if not neg_news.empty:
            neg_date = neg_news.iloc[0]["ë‚ ì§œ"]
            neg_title = neg_news.iloc[0]["ê¸°ì‚¬ì œëª©"]
            neg_link = neg_news.iloc[0]["ë§í¬"]
            neg_display = f'({neg_date}) {neg_title}'
            neg_hyperlink = f'=HYPERLINK("{neg_link}", "{neg_display}")'
        else:
            neg_hyperlink = ""

        result_rows.append({
            "ê¸°ì—…ëª…": company,
            "í‘œê¸°ëª…": excel_company_name,
            "ê¸ì • ë‰´ìŠ¤": pos_hyperlink,
            "ë¶€ì • ë‰´ìŠ¤": neg_hyperlink
        })

    df_result = pd.DataFrame(result_rows)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df_result.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
    output.seek(0)
    return output

def render_articles_with_single_summary_and_telegram(results, show_limit, show_sentiment_badge=True, enable_summary=True):
    SENTIMENT_CLASS = {
        "ê¸ì •": "sentiment-positive",
        "ë¶€ì •": "sentiment-negative"
    }

    if "article_checked" not in st.session_state:
        st.session_state.article_checked = {}

    col_list, col_summary = st.columns([1, 1])

    with col_list:
        st.markdown("### ê¸°ì‚¬ ìš”ì•½ ê²°ê³¼")
        for keyword, articles in results.items():
            with st.container(border=True):
                st.markdown(f"**[{keyword}]**")
                limit = st.session_state.show_limit.get(keyword, 5)
                for idx, article in enumerate(articles[:limit]):
                    unique_id = re.sub(r'\W+', '', article['link'])[-16:]
                    key = f"{keyword}_{idx}_{unique_id}"
                    sentiment_label = ""
                    sentiment_class = ""
                    sentiment_html = ""
                    if show_sentiment_badge:
                        if f"summary_{key}" in st.session_state:
                            _, _, sentiment, _ = st.session_state[f"summary_{key}"]
                            sentiment_label = sentiment if sentiment else "ë¶„ì„ì¤‘"
                            sentiment_class = SENTIMENT_CLASS.get(sentiment_label, "sentiment-negative")
                            sentiment_html = f"<span class='sentiment-badge {sentiment_class}'>({sentiment_label})</span>"
                    md_line = (
                        f"[{article['title']}]({article['link']}) "
                        f"{sentiment_html} "
                        f"{article['date']} | {article['source']}"
                    )
                    cols = st.columns([0.04, 0.96])
                    with cols[0]:
                        checked = st.checkbox("", value=st.session_state.article_checked.get(key, False), key=f"news_{key}")
                    with cols[1]:
                        st.markdown(md_line, unsafe_allow_html=True)
                    st.session_state.article_checked[key] = checked

                if limit < len(articles):
                    if st.button("ë”ë³´ê¸°", key=f"more_{keyword}"):
                        st.session_state.show_limit[keyword] += 10
                        st.rerun()

    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        with st.container(border=True):
            selected_articles = []
            def safe_title_for_append(val):
                if val is None or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
                    return "ì œëª©ì—†ìŒ"
                return str(val)
            for keyword, articles in results.items():
                limit = st.session_state.show_limit.get(keyword, 5)
                for idx, article in enumerate(articles[:limit]):
                    unique_id = re.sub(r'\W+', '', article['link'])[-16:]
                    key = f"{keyword}_{idx}_{unique_id}"
                    cache_key = f"summary_{key}"
                    if st.session_state.article_checked.get(key, False):
                        if cache_key in st.session_state:
                            one_line, summary, sentiment, full_text = st.session_state[cache_key]
                        else:
                            one_line, summary, sentiment, full_text = summarize_article_from_url(
                                article['link'], article['title'], do_summary=enable_summary
                            )
                            st.session_state[cache_key] = (one_line, summary, sentiment, full_text)
                        selected_articles.append({
                            "í‚¤ì›Œë“œ": keyword,
                            "ê¸°ì‚¬ì œëª©": safe_title_for_append(article.get('title')),
                            "ìš”ì•½": one_line,
                            "ìš”ì•½ë³¸": summary,
                            "ê°ì„±": sentiment,
                            "ë§í¬": article['link'],
                            "ë‚ ì§œ": article['date'],
                            "ì¶œì²˜": article['source']
                        })
                        if show_sentiment_badge:
                            st.markdown(
                                f"#### [{article['title']}]({article['link']}) "
                                f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-negative')}'>({sentiment})</span>",
                                unsafe_allow_html=True
                            )
                        else:
                            st.markdown(f"#### [{article['title']}]({article['link']})", unsafe_allow_html=True)
                        st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {article['date']} | {article['source']}")
                        if enable_summary:
                            st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {one_line}")
                        st.markdown(f"- **ê°ì„±ë¶„ì„:** `{sentiment}`")
                        st.markdown("---")

            st.session_state.selected_articles = selected_articles
            st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {len(selected_articles)}")

            excel_company_order = []
            for cat in ["êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ", "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"]:
                excel_company_order.extend(excel_company_categories.get(cat, []))

            if st.session_state.selected_articles:
                excel_bytes = get_excel_download_with_favorite_and_excel_company_col(
                    st.session_state.selected_articles,
                    favorite_categories,
                    excel_company_categories
                )
                st.download_button(
                    label="ğŸ“¥ ë§ì¶¤ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=excel_bytes.getvalue(),
                    file_name="ë‰´ìŠ¤ìš”ì•½_ë§ì¶¤í˜•.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

# ë‚ ì§œ ë³€ê²½ ì‹œ í•„í„°ë§
def filter_articles_by_date():
    st.session_state.filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered = [
            a for a in articles
            if st.session_state["start_date"] <= datetime.strptime(a['date'], "%Y-%m-%d").date() <= st.session_state["end_date"]
        ]
        if filtered:
            st.session_state.filtered_results[keyword] = filtered

# ë‚ ì§œ ìœ„ì ¯ ê°’ì´ ë°”ë€Œë©´ ìë™ í•„í„°ë§
if st.session_state.search_results:
    filter_articles_by_date()
    filtered_results = {}
    for keyword, articles in st.session_state.filtered_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        if filtered_articles:
            filtered_results[keyword] = filtered_articles
    render_articles_with_single_summary_and_telegram(
        filtered_results,
        st.session_state.show_limit,
        show_sentiment_badge=st.session_state.get("show_sentiment_badge", False),
        enable_summary=st.session_state.get("enable_summary", True)
    )

# --- ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ (ìºì‹±) ---
@st.cache_data(show_spinner=False)
def extract_article_text_cached(url):
    try:
        article = newspaper.Article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"

# --- OpenAI ìš”ì•½/ê°ì„±ë¶„ì„ í•¨ìˆ˜ (ìºì‹±) ---
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def detect_lang(text):
    return "ko" if re.search(r"[ê°€-í£]", text) else "en"

@st.cache_data(show_spinner=False)
def summarize_and_sentiment_with_openai_cached(text, title=None, do_summary=True):
    if not OPENAI_API_KEY:
        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, None, None

    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜¤ë¥˜ì¼ ë•Œ ìš”ì•½ ì‹œë„ ê¸ˆì§€
    if not text or "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜" in text or len(text.strip()) < 50:
        return "ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ìš”ì•½ ë¶ˆê°€", None, None, None

    lang = detect_lang(text)
    title = title or ""
    if lang == "ko":
        prompt = (
            f"ì•„ë˜ ê¸°ì‚¬ ì œëª©, ë³¸ë¬¸, ê·¸ë¦¬ê³  ê²€ìƒ‰ í‚¤ì›Œë“œ '{title}'ë¥¼ ì°¸ê³ í•´, ë°˜ë“œì‹œ í•œ ì¤„ ìš”ì•½ì— '{title}'ê°€ í¬í•¨ë˜ë„ë¡ í•´ì¤˜. "
            "ë§Œì•½ í‚¤ì›Œë“œì™€ ê´€ë ¨ ì—†ëŠ” ê¸°ì‚¬ë¼ë©´ 'í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë‚´ìš©ì´ ê¸°ì‚¬ì— ì—†ìŒ'ì´ë¼ê³  ë‹µí•´ì¤˜.\n"
            "- [í•œ ì¤„ ìš”ì•½]: ê¸°ì‚¬ ì œëª©, ë³¸ë¬¸, í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë°˜ë“œì‹œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\n"
            "- [ê°ì„±]: ê¸°ì‚¬ ì „ì²´ì˜ ê°ì •ì„ ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì¤˜. ì¤‘ë¦½ì€ ì ˆëŒ€ ë‹µí•˜ì§€ ë§ˆ. íŒŒì‚°, ìê¸ˆë‚œ ë“± ë¶€ì •ì  ì‚¬ê±´ì´ ì¤‘ì‹¬ì´ë©´ ë°˜ë“œì‹œ 'ë¶€ì •'ìœ¼ë¡œ ë‹µí•´ì¤˜.\n\n"
            "ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n"
            "[í•œ ì¤„ ìš”ì•½]: (ì—¬ê¸°ì— í•œ ì¤„ ìš”ì•½)\n"
            "[ê°ì„±]: (ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜ë§Œ)\n\n"
            f"[ê²€ìƒ‰ í‚¤ì›Œë“œ]\n{title}\n[ê¸°ì‚¬ ì œëª©]\n{title}\n[ê¸°ì‚¬ ë³¸ë¬¸]\n{text}"
        )
    else:
        prompt = (
        f"Summarize the following article in one sentence, and make sure the summary includes the keyword '{title}'. "
        "If the keyword is not relevant to the article, answer: 'No content related to the keyword.'\n"
        "- [One-line Summary]: Summarize with the keyword included.\n"
        "- [Sentiment]: positive or negative only.\n\n"
        "Respond in this format:\n"
        "[One-line Summary]: (your one-line summary)\n"
        "[Sentiment]: (positive/negative only)\n\n"
        f"[KEYWORD]\n{title}\n[TITLE]\n{title}\n[ARTICLE]\n{text}"
        )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": prompt}
        ],
        max_tokens=1024,
        temperature=0.3
    )
    answer = response.choices[0].message.content.strip()
    if lang == "ko":
        m1 = re.search(r"\[í•œ ì¤„ ìš”ì•½\]:\s*(.+)", answer)
        m3 = re.search(r"\[ê°ì„±\]:\s*(.+)", answer)
    else:
        m1 = re.search(r"\[One-line Summary\]:\s*(.+)", answer)
        m3 = re.search(r"\[Sentiment\]:\s*(.+)", answer)
    one_line = m1.group(1).strip() if (do_summary and m1) else ""
    summary = ""  # ìƒì„¸ ìš”ì•½ì€ ìƒëµ
    sentiment = m3.group(1).strip() if m3 else ""
    # í›„ì²˜ë¦¬: ì¤‘ë¦½ ë“± ë“¤ì–´ì˜¤ë©´ ë¶€ì •ìœ¼ë¡œ ê°•ì œ
    if sentiment.lower() in ['neutral', 'ì¤‘ë¦½', '']:
        sentiment = 'ë¶€ì •' if lang == "ko" else 'negative'
    if lang == "en":
        sentiment = 'ê¸ì •' if sentiment.lower() == 'positive' else 'ë¶€ì •'
    return one_line, summary, sentiment, text

def summarize_article_from_url(article_url, title, do_summary=True):
    try:
        full_text = extract_article_text_cached(article_url)
        if full_text.startswith("ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"):
            return full_text, None, None, None
        one_line, summary, sentiment, _ = summarize_and_sentiment_with_openai_cached(full_text, title=title, do_summary=do_summary)
        return one_line, summary, sentiment, full_text
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", None, None, None

NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
TELEGRAM_TOKEN = "7033950842:AAFk4pSb5qtNj435Gf2B5-rPlFrlNqhZFuQ"
TELEGRAM_CHAT_ID = "-1002404027768"

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(TELEGRAM_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_message(self, message):
        self.bot.sendMessage(self.chat_id, message, parse_mode="Markdown", disable_web_page_preview=True)

def filter_by_issues(title, desc, selected_keywords, require_keyword_in_title=False):
    if require_keyword_in_title and selected_keywords:
        if not any(kw.lower() in title.lower() for kw in selected_keywords):
            return False
    return True

def fetch_naver_news(query, start_date=None, end_date=None, limit=1000, require_keyword_in_title=False):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for start in range(1, 1001, 100):
        if len(articles) >= limit:
            break
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }
        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break
        items = response.json().get("items", [])
        for item in items:
            title, desc = item["title"], item["description"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            if start_date and pub_date < start_date:
                continue
            if end_date and pub_date > end_date:
                continue
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(re.sub("<.*?>", "", title), EXCLUDE_TITLE_KEYWORDS):
                continue
            articles.append({
                "title": re.sub("<.*?>", "", title),
                "description": re.sub("<.*?>", "", desc),
                "link": item["link"],
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "Naver"
            })
        if len(items) < 100:
            break
    return articles[:limit]

def fetch_gnews_news(query, start_date=None, end_date=None, limit=100, require_keyword_in_title=False):
    GNEWS_API_KEY = "b8c6d82bbdee9b61d2b9605f44ca8540"
    articles = []
    try:
        url = f"https://gnews.io/api/v4/search"
        params = {
            "q": query,
            "lang": "en",
            "token": GNEWS_API_KEY,
            "max": limit
        }
        response = requests.get(url, params=params)
        if response.status_code != 200:
            st.warning(f"âŒ GNews ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return []
        data = response.json()
        for item in data.get("articles", []):
            title = item.get("title", "")
            desc = item.get("description", "")
            if not filter_by_issues(title, desc, [query], require_keyword_in_title):
                continue
            if exclude_by_title_keywords(title, EXCLUDE_TITLE_KEYWORDS):
                continue
            pub_date = datetime.strptime(item["publishedAt"][:10], "%Y-%m-%d").date()
            articles.append({
                "title": title,
                "description": desc,
                "link": item.get("url", ""),
                "date": pub_date.strftime("%Y-%m-%d"),
                "source": "GNews"
            })
    except Exception as e:
        st.warning(f"âš ï¸ GNews ì ‘ê·¼ ì˜¤ë¥˜: {e}")
    return articles

def is_english(text):
    return all(ord(c) < 128 for c in text if c.isalpha())

def process_keywords(keyword_list, start_date, end_date, require_keyword_in_title=False):
    for k in keyword_list:
        if is_english(k):
            articles = fetch_gnews_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        else:
            articles = fetch_naver_news(k, start_date, end_date, require_keyword_in_title=require_keyword_in_title)
        articles = remove_duplicate_articles_by_title_and_keywords(articles, title_threshold=0.75, keyword_threshold=0.6)
        st.session_state.search_results[k] = articles
        if k not in st.session_state.show_limit:
            st.session_state.show_limit[k] = 5

def detect_lang_from_title(title):
    return "ko" if re.search(r"[ê°€-í£]", title) else "en"

def or_keyword_filter(article, *keyword_lists):
    text = (article.get("title", "") + " " + article.get("description", "") + " " + article.get("full_text", ""))
    for keywords in keyword_lists:
        if any(kw in text for kw in keywords if kw):
            return True
    return False

def article_contains_exact_keyword(article, keywords):
    title = article.get("title", "")
    content = ""
    cache_key = article.get("link", "")
    summary_cache_key = None
    for key in st.session_state.keys():
        if key.startswith("summary_") and cache_key in key:
            summary_cache_key = key
            break
    if summary_cache_key and isinstance(st.session_state[summary_cache_key], tuple):
        _, _, _, content = st.session_state[summary_cache_key]
    for kw in keywords:
        if kw and (kw in title or (content and kw in content)):
            return True
    return False

search_clicked = False
if keywords_input:
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        search_clicked = True

if search_clicked or st.session_state.get("search_triggered"):
    keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if len(keyword_list) > 10:
        st.warning("í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
            process_keywords(keyword_list, st.session_state["start_date"], st.session_state["end_date"], require_keyword_in_title=st.session_state.get("require_keyword_in_title", False))
    st.session_state.search_triggered = False

# ì‚°ì—…ë³„ ê²€ìƒ‰ ë²„íŠ¼ ë™ì‘ (ëŒ€ë¶„ë¥˜-ê¸°ì—…-ì´ìŠˆ êµ¬ì¡°)
if industry_search_clicked and selected_companies:
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        process_keywords(
            selected_companies,
            st.session_state["start_date"],
            st.session_state["end_date"],
            require_keyword_in_title=st.session_state.get("require_keyword_in_title", False)
        )
    # ì´ìŠˆëŠ” í›„ì²˜ë¦¬ í•„í„°ì—ì„œ ì ìš©

def article_passes_all_filters(article):
    filters = []
    filters.append(ALL_COMMON_FILTER_KEYWORDS)
    # ì‚°ì—…ë³„ ê²€ìƒ‰ì—ì„œ ì´ìŠˆ(OR) í•„í„° ì ìš©
    industry_issues_filter = st.session_state.get("industry_issues", [])
    if industry_issues_filter:
        filters.append(industry_issues_filter)
    # í™œì„±í™”ëœ ê³µí†µ í•„í„°ë§Œ ì ìš©
    if active_common_keywords:
        filters.append(active_common_keywords)
    if exclude_by_title_keywords(article.get('title', ''), EXCLUDE_TITLE_KEYWORDS):
        return False
    if st.session_state.get("require_exact_keyword_in_title_or_content", False):
        all_keywords = []
        if keywords_input:
            all_keywords.extend([k.strip() for k in keywords_input.split(",") if k.strip()])
        if not article_contains_exact_keyword(article, all_keywords):
            return False
    return or_keyword_filter(article, *filters)

def safe_title(val):
    if pd.isnull(val) or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
        return "ì œëª©ì—†ìŒ"
    return str(val)

def get_excel_download_with_favorite_and_excel_company_col(summary_data, favorite_categories, excel_company_categories):
    company_order = []
    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        company_order.extend(favorite_categories.get(cat, []))

    excel_company_order = []
    for cat in [
        "êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ",
        "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"
    ]:
        excel_company_order.extend(excel_company_categories.get(cat, []))

    df_articles = pd.DataFrame(summary_data)
    result_rows = []
    for idx, company in enumerate(company_order):
        excel_company_name = excel_company_order[idx] if idx < len(excel_company_order) else ""

        comp_articles = df_articles[df_articles["í‚¤ì›Œë“œ"] == company]
        pos_news = comp_articles[comp_articles["ê°ì„±"] == "ê¸ì •"].sort_values(by="ë‚ ì§œ", ascending=False)
        neg_news = comp_articles[comp_articles["ê°ì„±"] == "ë¶€ì •"].sort_values(by="ë‚ ì§œ", ascending=False)

        if not pos_news.empty:
            pos_date = pos_news.iloc[0]["ë‚ ì§œ"]
            pos_title = pos_news.iloc[0]["ê¸°ì‚¬ì œëª©"]
            pos_link = pos_news.iloc[0]["ë§í¬"]
            pos_display = f'({pos_date}) {pos_title}'
            pos_hyperlink = f'=HYPERLINK("{pos_link}", "{pos_display}")'
        else:
            pos_hyperlink = ""

        if not neg_news.empty:
            neg_date = neg_news.iloc[0]["ë‚ ì§œ"]
            neg_title = neg_news.iloc[0]["ê¸°ì‚¬ì œëª©"]
            neg_link = neg_news.iloc[0]["ë§í¬"]
            neg_display = f'({neg_date}) {neg_title}'
            neg_hyperlink = f'=HYPERLINK("{neg_link}", "{neg_display}")'
        else:
            neg_hyperlink = ""

        result_rows.append({
            "ê¸°ì—…ëª…": company,
            "í‘œê¸°ëª…": excel_company_name,
            "ê¸ì • ë‰´ìŠ¤": pos_hyperlink,
            "ë¶€ì • ë‰´ìŠ¤": neg_hyperlink
        })

    df_result = pd.DataFrame(result_rows)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df_result.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤ìš”ì•½')
    output.seek(0)
    return output

def render_articles_with_single_summary_and_telegram(results, show_limit, show_sentiment_badge=True, enable_summary=True):
    SENTIMENT_CLASS = {
        "ê¸ì •": "sentiment-positive",
        "ë¶€ì •": "sentiment-negative"
    }

    if "article_checked" not in st.session_state:
        st.session_state.article_checked = {}

    col_list, col_summary = st.columns([1, 1])

    with col_list:
        st.markdown("### ê¸°ì‚¬ ìš”ì•½ ê²°ê³¼")
        for keyword, articles in results.items():
            with st.container(border=True):
                st.markdown(f"**[{keyword}]**")
                limit = st.session_state.show_limit.get(keyword, 5)
                for idx, article in enumerate(articles[:limit]):
                    unique_id = re.sub(r'\W+', '', article['link'])[-16:]
                    key = f"{keyword}_{idx}_{unique_id}"
                    sentiment_label = ""
                    sentiment_class = ""
                    sentiment_html = ""
                    if show_sentiment_badge:
                        if f"summary_{key}" in st.session_state:
                            _, _, sentiment, _ = st.session_state[f"summary_{key}"]
                            sentiment_label = sentiment if sentiment else "ë¶„ì„ì¤‘"
                            sentiment_class = SENTIMENT_CLASS.get(sentiment_label, "sentiment-negative")
                            sentiment_html = f"<span class='sentiment-badge {sentiment_class}'>({sentiment_label})</span>"
                    md_line = (
                        f"[{article['title']}]({article['link']}) "
                        f"{sentiment_html} "
                        f"{article['date']} | {article['source']}"
                    )
                    cols = st.columns([0.04, 0.96])
                    with cols[0]:
                        checked = st.checkbox("", value=st.session_state.article_checked.get(key, False), key=f"news_{key}")
                    with cols[1]:
                        st.markdown(md_line, unsafe_allow_html=True)
                    st.session_state.article_checked[key] = checked

                if limit < len(articles):
                    if st.button("ë”ë³´ê¸°", key=f"more_{keyword}"):
                        st.session_state.show_limit[keyword] += 10
                        st.rerun()

    with col_summary:
        st.markdown("### ì„ íƒëœ ê¸°ì‚¬ ìš”ì•½/ê°ì„±ë¶„ì„")
        with st.container(border=True):
            selected_articles = []
            def safe_title_for_append(val):
                if val is None or str(val).strip() == "" or str(val).lower() == "nan" or str(val) == "0":
                    return "ì œëª©ì—†ìŒ"
                return str(val)
            for keyword, articles in results.items():
                limit = st.session_state.show_limit.get(keyword, 5)
                for idx, article in enumerate(articles[:limit]):
                    unique_id = re.sub(r'\W+', '', article['link'])[-16:]
                    key = f"{keyword}_{idx}_{unique_id}"
                    cache_key = f"summary_{key}"
                    if st.session_state.article_checked.get(key, False):
                        if cache_key in st.session_state:
                            one_line, summary, sentiment, full_text = st.session_state[cache_key]
                        else:
                            one_line, summary, sentiment, full_text = summarize_article_from_url(
                                article['link'], article['title'], do_summary=enable_summary
                            )
                            st.session_state[cache_key] = (one_line, summary, sentiment, full_text)
                        selected_articles.append({
                            "í‚¤ì›Œë“œ": keyword,
                            "ê¸°ì‚¬ì œëª©": safe_title_for_append(article.get('title')),
                            "ìš”ì•½": one_line,
                            "ìš”ì•½ë³¸": summary,
                            "ê°ì„±": sentiment,
                            "ë§í¬": article['link'],
                            "ë‚ ì§œ": article['date'],
                            "ì¶œì²˜": article['source']
                        })
                        if show_sentiment_badge:
                            st.markdown(
                                f"#### [{article['title']}]({article['link']}) "
                                f"<span class='sentiment-badge {SENTIMENT_CLASS.get(sentiment, 'sentiment-negative')}'>({sentiment})</span>",
                                unsafe_allow_html=True
                            )
                        else:
                            st.markdown(f"#### [{article['title']}]({article['link']})", unsafe_allow_html=True)
                        st.markdown(f"- **ë‚ ì§œ/ì¶œì²˜:** {article['date']} | {article['source']}")
                        if enable_summary:
                            st.markdown(f"- **í•œ ì¤„ ìš”ì•½:** {one_line}")
                        st.markdown(f"- **ê°ì„±ë¶„ì„:** `{sentiment}`")
                        st.markdown("---")

            st.session_state.selected_articles = selected_articles
            st.write(f"ì„ íƒëœ ê¸°ì‚¬ ê°œìˆ˜: {len(selected_articles)}")

            excel_company_order = []
            for cat in ["êµ­/ê³µì±„", "ê³µê³µê¸°ê´€", "ë³´í—˜ì‚¬", "5ëŒ€ê¸ˆìœµì§€ì£¼", "5ëŒ€ì‹œì¤‘ì€í–‰", "ì¹´ë“œì‚¬", "ìºí”¼íƒˆ", "ì§€ì£¼ì‚¬", "ì—ë„ˆì§€", "ë°œì „", "ìë™ì°¨", "ì „ê¸°/ì „ì", "ì†Œë¹„ì¬", "ë¹„ì² /ì² ê°•", "ì„ìœ í™”í•™", "ê±´ì„¤", "íŠ¹ìˆ˜ì±„"]:
                excel_company_order.extend(excel_company_categories.get(cat, []))

            if st.session_state.selected_articles:
                excel_bytes = get_excel_download_with_favorite_and_excel_company_col(
                    st.session_state.selected_articles,
                    favorite_categories,
                    excel_company_categories
                )
            st.download_button(
                label="ğŸ“¥ ë§ì¶¤ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                data=excel_bytes.getvalue(),
                file_name="ë‰´ìŠ¤ìš”ì•½_ë§ì¶¤í˜•.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

# ë‚ ì§œ ë³€ê²½ ì‹œ í•„í„°ë§
def filter_articles_by_date():
    st.session_state.filtered_results = {}
    for keyword, articles in st.session_state.search_results.items():
        filtered = [
            a for a in articles
            if st.session_state["start_date"] <= datetime.strptime(a['date'], "%Y-%m-%d").date() <= st.session_state["end_date"]
        ]
        if filtered:
            st.session_state.filtered_results[keyword] = filtered

# ë‚ ì§œ ìœ„ì ¯ ê°’ì´ ë°”ë€Œë©´ ìë™ í•„í„°ë§
if st.session_state.search_results:
    filter_articles_by_date()
    filtered_results = {}
    for keyword, articles in st.session_state.filtered_results.items():
        filtered_articles = [a for a in articles if article_passes_all_filters(a)]
        if filtered_articles:
            filtered_results[keyword] = filtered_articles
    render_articles_with_single_summary_and_telegram(
        filtered_results,
        st.session_state.show_limit,
        show_sentiment_badge=st.session_state.get("show_sentiment_badge", False),
        enable_summary=st.session_state.get("enable_summary", True)
    )
