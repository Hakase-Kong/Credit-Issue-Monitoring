import streamlit as st
import requests
import re
from datetime import datetime
from bs4 import BeautifulSoup
import pytz
import time
import concurrent.futures
import telepot

# --- API ÌÇ§ ÏÑ§Ï†ï ---
NAVER_CLIENT_ID = "_qXuzaBGk_jQesRRPRvu"
NAVER_CLIENT_SECRET = "lZc2gScgNq"
NEWS_API_KEY = "3a33b7b756274540926aeea8df60637c"

credit_keywords = ["\uc2e0\uc721\ub4f1\uae09", "\uc2e0\uc721\ud558\ud615", "\uc2e0\uc721\uc0c1\ud615", "\ub4f1\uae09\uc870\uc815", "\ubd88\uc815\uc801", "\uadc0\uc815\uc801", "\ud3c9\uac00"]
finance_keywords = ["\uc801\uc790", "\ud751\uc790", "\ubd80\uccb4", "\ucc28\uc785\uae08", "\ud604\uae08\ud718\ub839", "\uc601\uc5c5\uc190\uc2e4", "\uc21c\uc774\uc775", "\ubd80\ub3c4", "\ud30c\uc0b0"]
all_filter_keywords = sorted(set(credit_keywords + finance_keywords))

favorite_keywords = set()

class Telegram:
    def __init__(self):
        self.bot = telepot.Bot(token="YOUR_TELEGRAM_BOT_TOKEN")

    def sendMessage(self, message):
        self.bot.sendMessage("YOUR_TELEGRAM_CHAT_ID", message, parse_mode="Markdown")

def summarize_article(url):
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.find_all('p')
        content = ' '.join([p.get_text() for p in paragraphs])
        sentences = re.split(r'(?<=[.!?]) +', content)
        return ' '.join(sentences[:2]) if sentences else "\uc694\uc57d \uc5c6\uc74c"
    except:
        return "\uc694\uc57d \ubd88\uac00"

def filter_by_issues(title, desc, selected_keywords):
    content = title + " " + desc
    return all(re.search(k, content) for k in selected_keywords)

def fetch_naver_news(query, start_date=None, end_date=None, filters=None):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    articles = []
    for page in range(1, 6):
        params = {
            "query": query,
            "display": 10,
            "start": (page - 1) * 10 + 1,
            "sort": "date"
        }

        response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if response.status_code != 200:
            break

        items = response.json().get("items", [])
        if not items:
            break

        for item in items:
            title, desc = item["title"], item["description"]
            pub_date_obj = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z")
            if start_date and pub_date_obj.date() < start_date:
                continue
            if end_date and pub_date_obj.date() > end_date:
                continue
            if not re.search(rf"\\b{re.escape(query)}\\b", title + desc):
                continue
            if filters and not filter_by_issues(title, desc, filters):
                continue

            summary = summarize_article(item["link"])
            articles.append({
                "title": title,
                "link": item["link"],
                "pubDate": pub_date_obj.strftime("%Y-%m-%d %H:%M:%S"),
                "source": "Naver",
                "summary": summary
            })

    return articles

def fetch_newsapi_news(query, start_date=None, end_date=None, filters=None):
    articles = []
    for page in range(1, 6):
        params = {
            "q": query,
            "page": page,
            "pageSize": 20,
            "language": "ko",
            "sortBy": "publishedAt"
        }
        if start_date:
            params["from"] = start_date.isoformat()
        if end_date:
            params["to"] = end_date.isoformat()

        response = requests.get("https://newsapi.org/v2/everything", params=params,
                                headers={"Authorization": f"Bearer {NEWS_API_KEY}"})
        if response.status_code != 200:
            break

        items = response.json().get("articles", [])
        if not items:
            break

        for item in items:
            title, desc = item["title"], item["description"] or ""
            pub_date_obj = datetime.strptime(item["publishedAt"], '%Y-%m-%dT%H:%M:%SZ')
            if start_date and pub_date_obj.date() < start_date:
                continue
            if end_date and pub_date_obj.date() > end_date:
                continue
            if not re.search(rf"\\b{re.escape(query)}\\b", title + desc):
                continue
            if filters and not filter_by_issues(title, desc, filters):
                continue

            summary = summarize_article(item["url"])
            articles.append({
                "title": title,
                "link": item["url"],
                "pubDate": pub_date_obj.strftime("%Y-%m-%d %H:%M:%S"),
                "source": item["source"]["name"],
                "summary": summary
            })

    return articles

def render_articles(query, articles):
    if not articles:
        st.markdown(f"### ‚ùå '{query}' Í¥ÄÎ†® Îâ¥Ïä§ ÏóÜÏùå")
        return
    st.markdown(f"### üîé {query} Í¥ÄÎ†® Îâ¥Ïä§")
    for article in articles:
        st.markdown(f"- [{article['title']}]({article['link']})")
        st.caption(f"{article['pubDate']} | {article['source']}")
        st.write(f"_{article['summary']}_")

# --- Streamlit UI Íµ¨ÏÑ± ---
st.title("üìä Credit Issue Monitoring")

api_choice = st.selectbox("API ÏÑ†ÌÉù", ["Naver", "NewsAPI"])
keywords_input = st.text_input("üîç ÌÇ§ÏõåÎìú (Ïòà: ÏÇºÏÑ±, ÌïúÌôî)")
start_date = st.date_input("ÏãúÏûëÏùº", value=None)
end_date = st.date_input("Ï¢ÖÎ£åÏùº", value=None)
filters = st.multiselect("üìå ÌïÑÌÑ∞ÎßÅ ÌÇ§ÏõåÎìú ÏÑ†ÌÉù", all_filter_keywords)

# --- Ï¶êÍ≤®Ï∞æÍ∏∞ Í¥ÄÎ†® Í∏∞Îä• ---
if st.button("‚≠ê Ï¶êÍ≤®Ï∞æÍ∏∞ Ï∂îÍ∞Ä"):
    new_keywords = {kw.strip() for kw in keywords_input.split(",") if kw.strip()}
    favorite_keywords.update(new_keywords)
    st.success("Ï¶êÍ≤®Ï∞æÍ∏∞Ïóê Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§.")

fav_selected = st.multiselect("‚≠ê Ï¶êÍ≤®Ï∞æÍ∏∞ÏóêÏÑú Í≤ÄÏÉâ", sorted(list(favorite_keywords)))

if fav_selected:
    with st.spinner("Îâ¥Ïä§ Í≤ÄÏÉâ Ï§ë..."):
        for k in fav_selected:
            articles = fetch_naver_news(k, start_date, end_date, filters) if api_choice == "Naver" else fetch_newsapi_news(k, start_date, end_date, filters)
            render_articles(k, articles)

# --- ÏùºÎ∞ò ÌÇ§ÏõåÎìú Í≤ÄÏÉâ ---
if st.button("Í≤ÄÏÉâ"):
    if not keywords_input:
        st.warning("ÌÇ§ÏõåÎìúÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
    else:
        keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
        if len(keyword_list) > 10:
            st.error("‚ö†Ô∏è ÌÇ§ÏõåÎìúÎäî ÏµúÎåÄ 10Í∞úÍπåÏßÄ ÏûÖÎ†• Í∞ÄÎä•Ìï©ÎãàÎã§.")
        else:
            with st.spinner("Îâ¥Ïä§ Í≤ÄÏÉâ Ï§ë..."):
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    futures = []
                    for k in keyword_list:
                        if api_choice == "Naver":
                            futures.append(executor.submit(fetch_naver_news, k, start_date, end_date, filters))
                        else:
                            futures.append(executor.submit(fetch_newsapi_news, k, start_date, end_date, filters))

                    for k, future in zip(keyword_list, futures):
                        render_articles(k, future.result())
